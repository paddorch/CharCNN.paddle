nohup: ignoring input
W0824 13:13:46.355387 46832 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 11.4, Runtime API Version: 11.2
W0824 13:13:46.369784 46832 device_context.cc:422] device: 0, cuDNN Version: 8.0.
W0824 13:13:47.693516 46832 device_context.h:361] WARNING: device: 0. The installed Paddle is compiled with CUDNN 8.1, but CUDNN version in your machine is 8.0, which may cause serious incompatible bug. Please recompile or reinstall Paddle with compatible CUDNN version.
2.1.2

Loading data from /hy-tmp/datasets/yahoo_answers_csv/train.csv.split_train

Loading data from /hy-tmp/datasets/yahoo_answers_csv/train.csv.split_val

Number of training samples: 1329765
	Label 0:        132900
	Label 1:        132945
	Label 2:        132896
	Label 3:        133101
	Label 4:        132891
	Label 5:        132953
	Label 6:        133024
	Label 7:        132997
	Label 8:        133080
	Label 9:        132978

Number of developing samples: 70235
	Label 0:          7100
	Label 1:          7055
	Label 2:          7104
	Label 3:          6899
	Label 4:          7109
	Label 5:          7047
	Label 6:          6976
	Label 7:          7003
	Label 8:          6920
	Label 9:          7022
Directory already exists.

Configuration:
	Alphabet path:          config/alphabet.json
	Batch size:             128
	Checkpoint:             True
	Checkpoint per batch:   10000
	Class weight:           None
	Continue from:          /hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar
	Cuda:                   True
	Data augment:           True
	Decay factor:           0.5
	Device:                 None
	Dropout:                0.5
	Dynamic lr:             False
	Epochs:                 100
	Geo aug:                False
	Grad clip:              5
	Is small:               False
	Kernel num:             100
	Kernel sizes:           3,4,5
	L0:                     1014
	Log config:             True
	Log interval:           100
	Log result:             True
	Lr:                     0.0003
	Milestones:             [5, 10, 15]
	Num features:           70
	Num workers:            4
	Optimizer:              AdamW
	Save folder:            /hy-tmp/output/models_yahoo_answers
	Save interval:          5
	Shuffle:                True
	Train path:             /hy-tmp/datasets/yahoo_answers_csv/train.csv.split_train
	Val interval:           600
	Val path:               /hy-tmp/datasets/yahoo_answers_csv/train.csv.split_val
	Verbose:                False
CharCNN(
  (conv1): Sequential(
    (0): Conv1D(70, 1024, kernel_size=[7], data_format=NCL)
    (1): ReLU()
    (2): MaxPool1D(kernel_size=3, stride=3, padding=0)
  )
  (conv2): Sequential(
    (0): Conv1D(1024, 1024, kernel_size=[7], data_format=NCL)
    (1): ReLU()
    (2): MaxPool1D(kernel_size=3, stride=3, padding=0)
  )
  (conv3): Sequential(
    (0): Conv1D(1024, 1024, kernel_size=[3], data_format=NCL)
    (1): ReLU()
  )
  (conv4): Sequential(
    (0): Conv1D(1024, 1024, kernel_size=[3], data_format=NCL)
    (1): ReLU()
  )
  (conv5): Sequential(
    (0): Conv1D(1024, 1024, kernel_size=[3], data_format=NCL)
    (1): ReLU()
  )
  (conv6): Sequential(
    (0): Conv1D(1024, 1024, kernel_size=[3], data_format=NCL)
    (1): ReLU()
    (2): MaxPool1D(kernel_size=3, stride=3, padding=0)
  )
  (fc1): Sequential(
    (0): Linear(in_features=34816, out_features=2048, dtype=float32)
    (1): ReLU()
    (2): Dropout(p=0.5, axis=None, mode=upscale_in_train)
  )
  (fc2): Sequential(
    (0): Linear(in_features=2048, out_features=2048, dtype=float32)
    (1): ReLU()
    (2): Dropout(p=0.5, axis=None, mode=upscale_in_train)
  )
  (fc3): Linear(in_features=2048, out_features=10, dtype=float32)
  (log_softmax): LogSoftmax(axis=-1)
)
=> loading checkpoint from '/hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar'
=> checkpoint best acc: 70.84568886861314
/usr/lib/python3/dist-packages/urllib3/util/selectors.py:14: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import namedtuple, Mapping
/usr/lib/python3/dist-packages/urllib3/_collections.py:2: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working
  from collections import Mapping, MutableMapping
Epoch[11] Batch[100] - loss: 0.98887  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[200] - loss: 1.12400  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[300] - loss: 0.89718  lr: 0.00030  acc: 68.75% 88/128
Epoch[11] Batch[400] - loss: 0.91103  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[500] - loss: 0.92047  lr: 0.00030  acc: 70.31% 90/128
Epoch[11] Batch[600] - loss: 0.78317  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[700] - loss: 0.83422  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[800] - loss: 0.96428  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[900] - loss: 0.98463  lr: 0.00030  acc: 67.19% 86/128
Epoch[11] Batch[1000] - loss: 0.94299  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[1100] - loss: 0.77809  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[1200] - loss: 0.80097  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[1300] - loss: 0.87129  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[1400] - loss: 0.89003  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[1500] - loss: 0.92930  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[1600] - loss: 0.96240  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[1700] - loss: 0.85585  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[1800] - loss: 0.88960  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[1900] - loss: 0.97377  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[2000] - loss: 0.96140  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[2100] - loss: 0.79265  lr: 0.00030  acc: 77.34% 99/128
Epoch[11] Batch[2200] - loss: 0.90786  lr: 0.00030  acc: 68.75% 88/128
Epoch[11] Batch[2300] - loss: 0.87671  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[2400] - loss: 0.73446  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[2500] - loss: 0.94354  lr: 0.00030  acc: 73.44% 94/128
Epoch[11] Batch[2600] - loss: 0.78680  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[2700] - loss: 0.88924  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[2800] - loss: 0.88425  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[2900] - loss: 0.74772  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[3000] - loss: 0.90403  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[3100] - loss: 0.87708  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[3200] - loss: 0.76889  lr: 0.00030  acc: 78.12% 100/128
Epoch[11] Batch[3300] - loss: 0.77752  lr: 0.00030  acc: 77.34% 99/128
Epoch[11] Batch[3400] - loss: 0.80783  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[3500] - loss: 0.89608  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[3600] - loss: 0.97047  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[3700] - loss: 0.89251  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[3800] - loss: 0.91245  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[3900] - loss: 0.86953  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[4000] - loss: 0.90935  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[4100] - loss: 0.76482  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[4200] - loss: 0.84527  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[4300] - loss: 1.08333  lr: 0.00030  acc: 65.62% 84/128
Epoch[11] Batch[4400] - loss: 1.03046  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[4500] - loss: 1.00247  lr: 0.00030  acc: 64.06% 82/128
Epoch[11] Batch[4600] - loss: 0.82709  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[4700] - loss: 0.93593  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[4800] - loss: 0.91625  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[4900] - loss: 0.72349  lr: 0.00030  acc: 79.69% 102/128
Epoch[11] Batch[5000] - loss: 0.80750  lr: 0.00030  acc: 73.44% 94/128
Epoch[11] Batch[5100] - loss: 0.77420  lr: 0.00030  acc: 78.91% 101/128
Epoch[11] Batch[5200] - loss: 1.04467  lr: 0.00030  acc: 62.50% 80/128
Epoch[11] Batch[5300] - loss: 0.77123  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[5400] - loss: 0.82388  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[5500] - loss: 0.62513  lr: 0.00030  acc: 78.91% 101/128
Epoch[11] Batch[5600] - loss: 0.84670  lr: 0.00030  acc: 73.44% 94/128
Epoch[11] Batch[5700] - loss: 0.83960  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[5800] - loss: 1.00953  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[5900] - loss: 0.94616  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[6000] - loss: 0.83399  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[6100] - loss: 0.95701  lr: 0.00030  acc: 65.62% 84/128
Epoch[11] Batch[6200] - loss: 0.95194  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[6300] - loss: 0.76712  lr: 0.00030  acc: 79.69% 102/128
Epoch[11] Batch[6400] - loss: 0.86636  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[6500] - loss: 0.81367  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[6600] - loss: 0.74276  lr: 0.00030  acc: 75.00% 96/128
Epoch[11] Batch[6700] - loss: 0.82531  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[6800] - loss: 0.59462  lr: 0.00030  acc: 83.59% 107/128
Epoch[11] Batch[6900] - loss: 0.72088  lr: 0.00030  acc: 81.25% 104/128
Epoch[11] Batch[7000] - loss: 1.00856  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[7100] - loss: 0.79078  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[7200] - loss: 0.85768  lr: 0.00030  acc: 73.44% 94/128
Epoch[11] Batch[7300] - loss: 0.81415  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[7400] - loss: 0.84063  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[7500] - loss: 0.91916  lr: 0.00030  acc: 70.31% 90/128
Epoch[11] Batch[7600] - loss: 0.87534  lr: 0.00030  acc: 72.66% 93/128
Epoch[11] Batch[7700] - loss: 0.98652  lr: 0.00030  acc: 68.75% 88/128
Epoch[11] Batch[7800] - loss: 1.16840  lr: 0.00030  acc: 62.50% 80/128
Epoch[11] Batch[7900] - loss: 0.70806  lr: 0.00030  acc: 76.56% 98/128
Epoch[11] Batch[8000] - loss: 0.90756  lr: 0.00030  acc: 68.75% 88/128
Epoch[11] Batch[8100] - loss: 0.77774  lr: 0.00030  acc: 82.03% 105/128
Epoch[11] Batch[8200] - loss: 1.04313  lr: 0.00030  acc: 66.41% 85/128
Epoch[11] Batch[8300] - loss: 0.92248  lr: 0.00030  acc: 73.44% 94/128
Epoch[11] Batch[8400] - loss: 0.66739  lr: 0.00030  acc: 79.69% 102/128
Epoch[11] Batch[8500] - loss: 0.75134  lr: 0.00030  acc: 77.34% 99/128
Epoch[11] Batch[8600] - loss: 1.01688  lr: 0.00030  acc: 64.84% 83/128
Epoch[11] Batch[8700] - loss: 0.94291  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[8800] - loss: 0.96802  lr: 0.00030  acc: 67.97% 87/128
Epoch[11] Batch[8900] - loss: 0.84180  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[9000] - loss: 0.97925  lr: 0.00030  acc: 71.09% 91/128
Epoch[11] Batch[9100] - loss: 0.69352  lr: 0.00030  acc: 80.47% 103/128
Epoch[11] Batch[9200] - loss: 0.71580  lr: 0.00030  acc: 77.34% 99/128
Epoch[11] Batch[9300] - loss: 0.71868  lr: 0.00030  acc: 74.22% 95/128
Epoch[11] Batch[9400] - loss: 0.95520  lr: 0.00030  acc: 70.31% 90/128
Epoch[11] Batch[9500] - loss: 0.68678  lr: 0.00030  acc: 80.47% 103/128
Epoch[11] Batch[9600] - loss: 0.80006  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[9700] - loss: 0.95629  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[9800] - loss: 0.89580  lr: 0.00030  acc: 71.88% 92/128
Epoch[11] Batch[9900] - loss: 0.87644  lr: 0.00030  acc: 69.53% 89/128
Epoch[11] Batch[10000] - loss: 0.91483  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[10100] - loss: 0.85417  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[10200] - loss: 0.82406  lr: 0.00030  acc: 75.78% 97/128
Epoch[11] Batch[10300] - loss: 1.01694  lr: 0.00030  acc: 69.53% 89/128

Evaluation - loss: 0.00689  lr: 0.00030  acc: 72.00 (50507/70144) error: 28.00
Label: [31m  0[0m	Prec: [32m 65.8[0m% (3790/5759)  Recall: [32m 53.4[0m% (3790/7093)  F-Score: [32m 59.0[0m%[0m
Label: [31m  1[0m	Prec: [32m 70.8[0m% (5413/7644)  Recall: [32m 76.8[0m% (5413/7048)  F-Score: [32m 73.7[0m%[0m
Label: [31m  2[0m	Prec: [32m 68.9[0m% (6069/8805)  Recall: [32m 85.6[0m% (6069/7094)  F-Score: [32m 76.3[0m%[0m
Label: [31m  3[0m	Prec: [32m 59.3[0m% (3358/5661)  Recall: [32m 48.7[0m% (3358/6892)  F-Score: [32m 53.5[0m%[0m
Label: [31m  4[0m	Prec: [32m 83.2[0m% (6155/7402)  Recall: [32m 86.7[0m% (6155/7098)  F-Score: [32m 84.9[0m%[0m
Label: [31m  5[0m	Prec: [32m 87.2[0m% (6192/7103)  Recall: [32m 87.9[0m% (6192/7042)  F-Score: [32m 87.6[0m%[0m
Label: [31m  6[0m	Prec: [32m 62.8[0m% (3435/5469)  Recall: [32m 49.3[0m% (3435/6965)  F-Score: [32m 55.3[0m%[0m
Label: [31m  7[0m	Prec: [32m 69.8[0m% (5146/7373)  Recall: [32m 73.6[0m% (5146/6988)  F-Score: [32m 71.7[0m%[0m
Label: [31m  8[0m	Prec: [32m 74.6[0m% (5261/7054)  Recall: [32m 76.1[0m% (5261/6910)  F-Score: [32m 75.4[0m%[0m
Label: [31m  9[0m	Prec: [32m 72.2[0m% (5688/7874)  Recall: [32m 81.1[0m% (5688/7014)  F-Score: [32m 76.4[0m%[0m


=> found better validated model, saving to /hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar


Epoch[12] Batch[100] - loss: 0.77464  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[200] - loss: 0.77417  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[300] - loss: 0.81310  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[400] - loss: 0.88748  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[500] - loss: 0.80853  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[600] - loss: 0.90636  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[700] - loss: 0.74520  lr: 0.00030  acc: 77.34% 99/128
Epoch[12] Batch[800] - loss: 0.82998  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[900] - loss: 0.86305  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[1000] - loss: 0.77670  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[1100] - loss: 0.71387  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[1200] - loss: 0.72584  lr: 0.00030  acc: 78.91% 101/128
Epoch[12] Batch[1300] - loss: 0.83251  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[1400] - loss: 0.86131  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[1500] - loss: 0.75767  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[1600] - loss: 0.73928  lr: 0.00030  acc: 79.69% 102/128
Epoch[12] Batch[1700] - loss: 0.63698  lr: 0.00030  acc: 79.69% 102/128
Epoch[12] Batch[1800] - loss: 0.81913  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[1900] - loss: 0.64612  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[2000] - loss: 0.75175  lr: 0.00030  acc: 80.47% 103/128
Epoch[12] Batch[2100] - loss: 0.68081  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[2200] - loss: 0.75924  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[2300] - loss: 0.79048  lr: 0.00030  acc: 79.69% 102/128
Epoch[12] Batch[2400] - loss: 0.96928  lr: 0.00030  acc: 70.31% 90/128
Epoch[12] Batch[2500] - loss: 0.98814  lr: 0.00030  acc: 67.97% 87/128
Epoch[12] Batch[2600] - loss: 0.90464  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[2700] - loss: 0.87153  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[2800] - loss: 1.08394  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[2900] - loss: 1.00882  lr: 0.00030  acc: 69.53% 89/128
Epoch[12] Batch[3000] - loss: 0.82107  lr: 0.00030  acc: 69.53% 89/128
Epoch[12] Batch[3100] - loss: 0.82292  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[3200] - loss: 0.75116  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[3300] - loss: 0.78113  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[3400] - loss: 0.80527  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[3500] - loss: 0.93307  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[3600] - loss: 0.85072  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[3700] - loss: 0.95584  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[3800] - loss: 1.08859  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[3900] - loss: 0.77361  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[4000] - loss: 0.79331  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[4100] - loss: 0.88470  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[4200] - loss: 0.67614  lr: 0.00030  acc: 78.91% 101/128
Epoch[12] Batch[4300] - loss: 0.84593  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[4400] - loss: 0.91800  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[4500] - loss: 0.72697  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[4600] - loss: 1.10572  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[4700] - loss: 0.79597  lr: 0.00030  acc: 77.34% 99/128
Epoch[12] Batch[4800] - loss: 0.93949  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[4900] - loss: 0.90209  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[5000] - loss: 0.86407  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[5100] - loss: 0.85927  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[5200] - loss: 0.88836  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[5300] - loss: 0.96897  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[5400] - loss: 1.01805  lr: 0.00030  acc: 70.31% 90/128
Epoch[12] Batch[5500] - loss: 1.06372  lr: 0.00030  acc: 70.31% 90/128
Epoch[12] Batch[5600] - loss: 0.73412  lr: 0.00030  acc: 78.12% 100/128
Epoch[12] Batch[5700] - loss: 0.75502  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[5800] - loss: 0.67470  lr: 0.00030  acc: 82.81% 106/128
Epoch[12] Batch[5900] - loss: 0.80608  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[6000] - loss: 0.69366  lr: 0.00030  acc: 78.91% 101/128
Epoch[12] Batch[6100] - loss: 0.81590  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[6200] - loss: 0.85482  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[6300] - loss: 0.83585  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[6400] - loss: 0.99258  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[6500] - loss: 1.01839  lr: 0.00030  acc: 65.62% 84/128
Epoch[12] Batch[6600] - loss: 0.97284  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[6700] - loss: 0.76885  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[6800] - loss: 0.88019  lr: 0.00030  acc: 70.31% 90/128
Epoch[12] Batch[6900] - loss: 1.01986  lr: 0.00030  acc: 65.62% 84/128
Epoch[12] Batch[7000] - loss: 0.98154  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[7100] - loss: 1.02744  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[7200] - loss: 0.93361  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[7300] - loss: 0.82222  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[7400] - loss: 0.80681  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[7500] - loss: 0.91329  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[7600] - loss: 0.89234  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[7700] - loss: 0.87143  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[7800] - loss: 0.77032  lr: 0.00030  acc: 72.66% 93/128
Epoch[12] Batch[7900] - loss: 0.82880  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[8000] - loss: 1.04427  lr: 0.00030  acc: 69.53% 89/128
Epoch[12] Batch[8100] - loss: 1.21226  lr: 0.00030  acc: 67.19% 86/128
Epoch[12] Batch[8200] - loss: 0.67712  lr: 0.00030  acc: 79.69% 102/128
Epoch[12] Batch[8300] - loss: 0.84424  lr: 0.00030  acc: 77.34% 99/128
Epoch[12] Batch[8400] - loss: 0.92608  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[8500] - loss: 0.82337  lr: 0.00030  acc: 76.56% 98/128
Epoch[12] Batch[8600] - loss: 0.80309  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[8700] - loss: 0.97673  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[8800] - loss: 1.07465  lr: 0.00030  acc: 66.41% 85/128
Epoch[12] Batch[8900] - loss: 0.85578  lr: 0.00030  acc: 73.44% 94/128
Epoch[12] Batch[9000] - loss: 0.93148  lr: 0.00030  acc: 67.97% 87/128
Epoch[12] Batch[9100] - loss: 0.97962  lr: 0.00030  acc: 69.53% 89/128
Epoch[12] Batch[9200] - loss: 0.68711  lr: 0.00030  acc: 80.47% 103/128
Epoch[12] Batch[9300] - loss: 1.05117  lr: 0.00030  acc: 59.38% 76/128
Epoch[12] Batch[9400] - loss: 0.96244  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[9500] - loss: 0.82259  lr: 0.00030  acc: 74.22% 95/128
Epoch[12] Batch[9600] - loss: 0.87428  lr: 0.00030  acc: 69.53% 89/128
Epoch[12] Batch[9700] - loss: 0.77145  lr: 0.00030  acc: 75.00% 96/128
Epoch[12] Batch[9800] - loss: 0.91786  lr: 0.00030  acc: 68.75% 88/128
Epoch[12] Batch[9900] - loss: 0.68565  lr: 0.00030  acc: 75.78% 97/128
Epoch[12] Batch[10000] - loss: 0.94544  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[10100] - loss: 0.93525  lr: 0.00030  acc: 71.88% 92/128
Epoch[12] Batch[10200] - loss: 1.02085  lr: 0.00030  acc: 71.09% 91/128
Epoch[12] Batch[10300] - loss: 0.92603  lr: 0.00030  acc: 75.78% 97/128

Evaluation - loss: 0.00684  lr: 0.00030  acc: 72.07 (50555/70144) error: 27.93
Label: [31m  0[0m	Prec: [32m 66.8[0m% (3752/5616)  Recall: [32m 52.9[0m% (3752/7090)  F-Score: [32m 59.1[0m%[0m
Label: [31m  1[0m	Prec: [32m 68.9[0m% (5550/8060)  Recall: [32m 78.8[0m% (5550/7047)  F-Score: [32m 73.5[0m%[0m
Label: [31m  2[0m	Prec: [32m 70.3[0m% (6014/8550)  Recall: [32m 84.7[0m% (6014/7097)  F-Score: [32m 76.9[0m%[0m
Label: [31m  3[0m	Prec: [32m 62.2[0m% (3202/5152)  Recall: [32m 46.5[0m% (3202/6891)  F-Score: [32m 53.2[0m%[0m
Label: [31m  4[0m	Prec: [32m 82.3[0m% (6255/7598)  Recall: [32m 88.1[0m% (6255/7099)  F-Score: [32m 85.1[0m%[0m
Label: [31m  5[0m	Prec: [32m 88.9[0m% (6108/6871)  Recall: [32m 86.8[0m% (6108/7039)  F-Score: [32m 87.8[0m%[0m
Label: [31m  6[0m	Prec: [32m 64.8[0m% (3357/5184)  Recall: [32m 48.2[0m% (3357/6968)  F-Score: [32m 55.3[0m%[0m
Label: [31m  7[0m	Prec: [32m 70.5[0m% (5134/7282)  Recall: [32m 73.4[0m% (5134/6992)  F-Score: [32m 71.9[0m%[0m
Label: [31m  8[0m	Prec: [32m 67.6[0m% (5642/8348)  Recall: [32m 81.6[0m% (5642/6914)  F-Score: [32m 73.9[0m%[0m
Label: [31m  9[0m	Prec: [32m 74.0[0m% (5541/7483)  Recall: [32m 79.1[0m% (5541/7007)  F-Score: [32m 76.5[0m%[0m


=> found better validated model, saving to /hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar


Epoch[13] Batch[100] - loss: 0.79480  lr: 0.00030  acc: 69.53% 89/128
Epoch[13] Batch[200] - loss: 1.00483  lr: 0.00030  acc: 70.31% 90/128
Epoch[13] Batch[300] - loss: 0.92454  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[400] - loss: 0.88320  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[500] - loss: 0.66926  lr: 0.00030  acc: 78.12% 100/128
Epoch[13] Batch[600] - loss: 0.86087  lr: 0.00030  acc: 67.97% 87/128
Epoch[13] Batch[700] - loss: 0.70927  lr: 0.00030  acc: 80.47% 103/128
Epoch[13] Batch[800] - loss: 0.76046  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[900] - loss: 0.83196  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[1000] - loss: 0.86107  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[1100] - loss: 0.82551  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[1200] - loss: 0.85649  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[1300] - loss: 0.92776  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[1400] - loss: 0.74011  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[1500] - loss: 0.76940  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[1600] - loss: 0.96578  lr: 0.00030  acc: 71.09% 91/128
Epoch[13] Batch[1700] - loss: 0.89337  lr: 0.00030  acc: 71.09% 91/128
Epoch[13] Batch[1800] - loss: 0.86094  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[1900] - loss: 0.78009  lr: 0.00030  acc: 78.12% 100/128
Epoch[13] Batch[2000] - loss: 0.97363  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[2100] - loss: 0.54307  lr: 0.00030  acc: 85.16% 109/128
Epoch[13] Batch[2200] - loss: 0.71479  lr: 0.00030  acc: 78.12% 100/128
Epoch[13] Batch[2300] - loss: 0.80623  lr: 0.00030  acc: 78.12% 100/128
Epoch[13] Batch[2400] - loss: 0.79931  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[2500] - loss: 0.94421  lr: 0.00030  acc: 70.31% 90/128
Epoch[13] Batch[2600] - loss: 0.87372  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[2700] - loss: 0.74264  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[2800] - loss: 0.77292  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[2900] - loss: 0.74945  lr: 0.00030  acc: 81.25% 104/128
Epoch[13] Batch[3000] - loss: 0.84271  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[3100] - loss: 0.82311  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[3200] - loss: 0.86374  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[3300] - loss: 0.79786  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[3400] - loss: 0.85330  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[3500] - loss: 0.80068  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[3600] - loss: 0.76901  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[3700] - loss: 0.87526  lr: 0.00030  acc: 69.53% 89/128
Epoch[13] Batch[3800] - loss: 0.87033  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[3900] - loss: 0.63958  lr: 0.00030  acc: 78.91% 101/128
Epoch[13] Batch[4000] - loss: 0.75787  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[4100] - loss: 0.80983  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[4200] - loss: 0.99535  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[4300] - loss: 0.78657  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[4400] - loss: 0.93986  lr: 0.00030  acc: 64.06% 82/128
Epoch[13] Batch[4500] - loss: 0.85348  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[4600] - loss: 0.77326  lr: 0.00030  acc: 77.34% 99/128
Epoch[13] Batch[4700] - loss: 0.78737  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[4800] - loss: 0.79595  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[4900] - loss: 0.76794  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[5000] - loss: 0.76285  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[5100] - loss: 0.94018  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[5200] - loss: 0.75330  lr: 0.00030  acc: 69.53% 89/128
Epoch[13] Batch[5300] - loss: 0.73572  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[5400] - loss: 0.74031  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[5500] - loss: 0.81105  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[5600] - loss: 0.84645  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[5700] - loss: 0.65714  lr: 0.00030  acc: 79.69% 102/128
Epoch[13] Batch[5800] - loss: 0.81433  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[5900] - loss: 0.82571  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[6000] - loss: 0.80280  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[6100] - loss: 0.70325  lr: 0.00030  acc: 81.25% 104/128
Epoch[13] Batch[6200] - loss: 0.89735  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[6300] - loss: 0.64847  lr: 0.00030  acc: 77.34% 99/128
Epoch[13] Batch[6400] - loss: 0.74164  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[6500] - loss: 0.72590  lr: 0.00030  acc: 78.12% 100/128
Epoch[13] Batch[6600] - loss: 0.98141  lr: 0.00030  acc: 67.19% 86/128
Epoch[13] Batch[6700] - loss: 0.78647  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[6800] - loss: 0.78078  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[6900] - loss: 0.89771  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[7000] - loss: 1.03493  lr: 0.00030  acc: 67.19% 86/128
Epoch[13] Batch[7100] - loss: 0.84061  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[7200] - loss: 0.74114  lr: 0.00030  acc: 79.69% 102/128
Epoch[13] Batch[7300] - loss: 0.99762  lr: 0.00030  acc: 67.19% 86/128
Epoch[13] Batch[7400] - loss: 0.87428  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[7500] - loss: 0.74472  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[7600] - loss: 0.80382  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[7700] - loss: 0.74846  lr: 0.00030  acc: 74.22% 95/128
Epoch[13] Batch[7800] - loss: 0.92756  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[7900] - loss: 0.82796  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[8000] - loss: 0.83906  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[8100] - loss: 0.85626  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[8200] - loss: 0.93405  lr: 0.00030  acc: 70.31% 90/128
Epoch[13] Batch[8300] - loss: 0.97247  lr: 0.00030  acc: 71.09% 91/128
Epoch[13] Batch[8400] - loss: 0.97366  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[8500] - loss: 0.92251  lr: 0.00030  acc: 69.53% 89/128
Epoch[13] Batch[8600] - loss: 0.87699  lr: 0.00030  acc: 71.09% 91/128
Epoch[13] Batch[8700] - loss: 0.74046  lr: 0.00030  acc: 73.44% 94/128
Epoch[13] Batch[8800] - loss: 0.86183  lr: 0.00030  acc: 67.97% 87/128
Epoch[13] Batch[8900] - loss: 0.93913  lr: 0.00030  acc: 68.75% 88/128
Epoch[13] Batch[9000] - loss: 0.78143  lr: 0.00030  acc: 71.88% 92/128
Epoch[13] Batch[9100] - loss: 0.98150  lr: 0.00030  acc: 64.84% 83/128
Epoch[13] Batch[9200] - loss: 0.73440  lr: 0.00030  acc: 75.78% 97/128
Epoch[13] Batch[9300] - loss: 0.90133  lr: 0.00030  acc: 68.75% 88/128
Epoch[13] Batch[9400] - loss: 0.92356  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[9500] - loss: 0.77486  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[9600] - loss: 0.82047  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[9700] - loss: 0.85128  lr: 0.00030  acc: 70.31% 90/128
Epoch[13] Batch[9800] - loss: 0.85659  lr: 0.00030  acc: 72.66% 93/128
Epoch[13] Batch[9900] - loss: 0.64270  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[10000] - loss: 0.76708  lr: 0.00030  acc: 76.56% 98/128
Epoch[13] Batch[10100] - loss: 1.08707  lr: 0.00030  acc: 62.50% 80/128
Epoch[13] Batch[10200] - loss: 0.87548  lr: 0.00030  acc: 75.00% 96/128
Epoch[13] Batch[10300] - loss: 0.80952  lr: 0.00030  acc: 73.44% 94/128

Evaluation - loss: 0.00682  lr: 0.00030  acc: 72.40 (50782/70144) error: 27.60
Label: [31m  0[0m	Prec: [32m 60.8[0m% (4197/6906)  Recall: [32m 59.2[0m% (4197/7089)  F-Score: [32m 60.0[0m%[0m
Label: [31m  1[0m	Prec: [32m 70.5[0m% (5527/7835)  Recall: [32m 78.5[0m% (5527/7045)  F-Score: [32m 74.3[0m%[0m
Label: [31m  2[0m	Prec: [32m 74.8[0m% (5843/7808)  Recall: [32m 82.4[0m% (5843/7090)  F-Score: [32m 78.4[0m%[0m
Label: [31m  3[0m	Prec: [32m 63.7[0m% (3154/4955)  Recall: [32m 45.8[0m% (3154/6889)  F-Score: [32m 53.3[0m%[0m
Label: [31m  4[0m	Prec: [32m 82.0[0m% (6284/7666)  Recall: [32m 88.5[0m% (6284/7100)  F-Score: [32m 85.1[0m%[0m
Label: [31m  5[0m	Prec: [32m 83.4[0m% (6351/7611)  Recall: [32m 90.2[0m% (6351/7038)  F-Score: [32m 86.7[0m%[0m
Label: [31m  6[0m	Prec: [32m 63.5[0m% (3437/5410)  Recall: [32m 49.3[0m% (3437/6966)  F-Score: [32m 55.5[0m%[0m
Label: [31m  7[0m	Prec: [32m 69.7[0m% (5127/7353)  Recall: [32m 73.3[0m% (5127/6998)  F-Score: [32m 71.5[0m%[0m
Label: [31m  8[0m	Prec: [32m 73.1[0m% (5428/7424)  Recall: [32m 78.5[0m% (5428/6916)  F-Score: [32m 75.7[0m%[0m
Label: [31m  9[0m	Prec: [32m 75.7[0m% (5434/7176)  Recall: [32m 77.5[0m% (5434/7013)  F-Score: [32m 76.6[0m%[0m


=> found better validated model, saving to /hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar


Epoch[14] Batch[100] - loss: 0.69696  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[200] - loss: 0.78017  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[300] - loss: 0.65056  lr: 0.00030  acc: 82.03% 105/128
Epoch[14] Batch[400] - loss: 0.75801  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[500] - loss: 0.87549  lr: 0.00030  acc: 68.75% 88/128
Epoch[14] Batch[600] - loss: 0.59760  lr: 0.00030  acc: 79.69% 102/128
Epoch[14] Batch[700] - loss: 0.83413  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[800] - loss: 1.00168  lr: 0.00030  acc: 64.84% 83/128
Epoch[14] Batch[900] - loss: 0.73920  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[1000] - loss: 0.72370  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[1100] - loss: 0.72144  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[1200] - loss: 0.78390  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[1300] - loss: 0.64327  lr: 0.00030  acc: 79.69% 102/128
Epoch[14] Batch[1400] - loss: 0.82874  lr: 0.00030  acc: 73.44% 94/128
Epoch[14] Batch[1500] - loss: 0.71557  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[1600] - loss: 0.67851  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[1700] - loss: 0.74273  lr: 0.00030  acc: 76.56% 98/128
Epoch[14] Batch[1800] - loss: 0.76171  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[1900] - loss: 0.84832  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[2000] - loss: 0.95582  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[2100] - loss: 0.80264  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[2200] - loss: 0.76704  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[2300] - loss: 0.79281  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[2400] - loss: 0.98254  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[2500] - loss: 0.80465  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[2600] - loss: 0.70490  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[2700] - loss: 0.76122  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[2800] - loss: 0.78150  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[2900] - loss: 0.93934  lr: 0.00030  acc: 68.75% 88/128
Epoch[14] Batch[3000] - loss: 0.76743  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[3100] - loss: 0.88042  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[3200] - loss: 0.91680  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[3300] - loss: 0.74688  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[3400] - loss: 0.85335  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[3500] - loss: 0.81235  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[3600] - loss: 0.74196  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[3700] - loss: 0.89356  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[3800] - loss: 0.88782  lr: 0.00030  acc: 70.31% 90/128
Epoch[14] Batch[3900] - loss: 0.84620  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[4000] - loss: 0.75187  lr: 0.00030  acc: 76.56% 98/128
Epoch[14] Batch[4100] - loss: 0.88850  lr: 0.00030  acc: 70.31% 90/128
Epoch[14] Batch[4200] - loss: 0.73435  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[4300] - loss: 0.70524  lr: 0.00030  acc: 76.56% 98/128
Epoch[14] Batch[4400] - loss: 0.71041  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[4500] - loss: 0.79909  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[4600] - loss: 0.62321  lr: 0.00030  acc: 82.81% 106/128
Epoch[14] Batch[4700] - loss: 0.66860  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[4800] - loss: 0.83136  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[4900] - loss: 0.63643  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[5000] - loss: 0.87916  lr: 0.00030  acc: 70.31% 90/128
Epoch[14] Batch[5100] - loss: 0.91614  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[5200] - loss: 0.94966  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[5300] - loss: 0.79752  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[5400] - loss: 0.82364  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[5500] - loss: 0.72336  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[5600] - loss: 0.85126  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[5700] - loss: 0.80003  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[5800] - loss: 0.70342  lr: 0.00030  acc: 81.25% 104/128
Epoch[14] Batch[5900] - loss: 0.80896  lr: 0.00030  acc: 73.44% 94/128
Epoch[14] Batch[6000] - loss: 0.64117  lr: 0.00030  acc: 84.38% 108/128
Epoch[14] Batch[6100] - loss: 0.78567  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[6200] - loss: 0.87925  lr: 0.00030  acc: 70.31% 90/128
Epoch[14] Batch[6300] - loss: 0.95803  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[6400] - loss: 0.77128  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[6500] - loss: 0.69745  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[6600] - loss: 0.83873  lr: 0.00030  acc: 69.53% 89/128
Epoch[14] Batch[6700] - loss: 0.74862  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[6800] - loss: 0.88089  lr: 0.00030  acc: 73.44% 94/128
Epoch[14] Batch[6900] - loss: 0.57806  lr: 0.00030  acc: 78.91% 101/128
Epoch[14] Batch[7000] - loss: 0.66172  lr: 0.00030  acc: 76.56% 98/128
Epoch[14] Batch[7100] - loss: 0.77826  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[7200] - loss: 1.03864  lr: 0.00030  acc: 67.19% 86/128
Epoch[14] Batch[7300] - loss: 0.73129  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[7400] - loss: 0.73847  lr: 0.00030  acc: 79.69% 102/128
Epoch[14] Batch[7500] - loss: 0.89231  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[7600] - loss: 0.83759  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[7700] - loss: 0.73407  lr: 0.00030  acc: 81.25% 104/128
Epoch[14] Batch[7800] - loss: 0.68616  lr: 0.00030  acc: 81.25% 104/128
Epoch[14] Batch[7900] - loss: 0.80966  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[8000] - loss: 0.96332  lr: 0.00030  acc: 70.31% 90/128
Epoch[14] Batch[8100] - loss: 0.87524  lr: 0.00030  acc: 68.75% 88/128
Epoch[14] Batch[8200] - loss: 0.76606  lr: 0.00030  acc: 75.00% 96/128
Epoch[14] Batch[8300] - loss: 0.76117  lr: 0.00030  acc: 73.44% 94/128
Epoch[14] Batch[8400] - loss: 0.78447  lr: 0.00030  acc: 76.56% 98/128
Epoch[14] Batch[8500] - loss: 0.70654  lr: 0.00030  acc: 78.12% 100/128
Epoch[14] Batch[8600] - loss: 0.90410  lr: 0.00030  acc: 71.09% 91/128
Epoch[14] Batch[8700] - loss: 0.83237  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[8800] - loss: 0.87243  lr: 0.00030  acc: 71.88% 92/128
Epoch[14] Batch[8900] - loss: 0.84264  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[9000] - loss: 0.79218  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[9100] - loss: 0.77323  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[9200] - loss: 0.72352  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[9300] - loss: 0.69768  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[9400] - loss: 1.02867  lr: 0.00030  acc: 67.19% 86/128
Epoch[14] Batch[9500] - loss: 0.69595  lr: 0.00030  acc: 79.69% 102/128
Epoch[14] Batch[9600] - loss: 0.73612  lr: 0.00030  acc: 77.34% 99/128
Epoch[14] Batch[9700] - loss: 0.78138  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[9800] - loss: 0.84821  lr: 0.00030  acc: 74.22% 95/128
Epoch[14] Batch[9900] - loss: 0.99060  lr: 0.00030  acc: 66.41% 85/128
Epoch[14] Batch[10000] - loss: 0.83557  lr: 0.00030  acc: 72.66% 93/128
Epoch[14] Batch[10100] - loss: 1.13275  lr: 0.00030  acc: 64.06% 82/128
Epoch[14] Batch[10200] - loss: 0.75875  lr: 0.00030  acc: 75.78% 97/128
Epoch[14] Batch[10300] - loss: 0.80006  lr: 0.00030  acc: 74.22% 95/128

Evaluation - loss: 0.00677  lr: 0.00030  acc: 72.45 (50817/70144) error: 27.55
Label: [31m  0[0m	Prec: [32m 59.4[0m% (4204/7080)  Recall: [32m 59.3[0m% (4204/7090)  F-Score: [32m 59.3[0m%[0m
Label: [31m  1[0m	Prec: [32m 73.0[0m% (5368/7349)  Recall: [32m 76.1[0m% (5368/7051)  F-Score: [32m 74.6[0m%[0m
Label: [31m  2[0m	Prec: [32m 75.2[0m% (5837/7761)  Recall: [32m 82.3[0m% (5837/7092)  F-Score: [32m 78.6[0m%[0m
Label: [31m  3[0m	Prec: [32m 58.3[0m% (3484/5981)  Recall: [32m 50.6[0m% (3484/6890)  F-Score: [32m 54.1[0m%[0m
Label: [31m  4[0m	Prec: [32m 81.1[0m% (6322/7798)  Recall: [32m 89.0[0m% (6322/7101)  F-Score: [32m 84.9[0m%[0m
Label: [31m  5[0m	Prec: [32m 87.7[0m% (6187/7055)  Recall: [32m 87.9[0m% (6187/7035)  F-Score: [32m 87.8[0m%[0m
Label: [31m  6[0m	Prec: [32m 66.3[0m% (3249/4903)  Recall: [32m 46.6[0m% (3249/6969)  F-Score: [32m 54.7[0m%[0m
Label: [31m  7[0m	Prec: [32m 72.7[0m% (5059/6961)  Recall: [32m 72.3[0m% (5059/6998)  F-Score: [32m 72.5[0m%[0m
Label: [31m  8[0m	Prec: [32m 71.6[0m% (5506/7691)  Recall: [32m 79.7[0m% (5506/6908)  F-Score: [32m 75.4[0m%[0m
Label: [31m  9[0m	Prec: [32m 74.0[0m% (5601/7565)  Recall: [32m 79.9[0m% (5601/7010)  F-Score: [32m 76.9[0m%[0m


=> found better validated model, saving to /hy-tmp/output/models_yahoo_answers/CharCNN_best.pth.tar


Epoch[15] Batch[100] - loss: 0.67249  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[200] - loss: 0.67569  lr: 0.00030  acc: 82.03% 105/128
Epoch[15] Batch[300] - loss: 0.63846  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[400] - loss: 0.82755  lr: 0.00030  acc: 75.78% 97/128
Epoch[15] Batch[500] - loss: 0.75839  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[600] - loss: 0.65397  lr: 0.00030  acc: 80.47% 103/128
Epoch[15] Batch[700] - loss: 0.67356  lr: 0.00030  acc: 80.47% 103/128
Epoch[15] Batch[800] - loss: 0.78251  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[900] - loss: 0.69158  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[1000] - loss: 0.71600  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[1100] - loss: 0.79371  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[1200] - loss: 0.97392  lr: 0.00030  acc: 72.66% 93/128
Epoch[15] Batch[1300] - loss: 0.83379  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[1400] - loss: 0.77947  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[1500] - loss: 0.58100  lr: 0.00030  acc: 83.59% 107/128
Epoch[15] Batch[1600] - loss: 0.66041  lr: 0.00030  acc: 78.12% 100/128
Epoch[15] Batch[1700] - loss: 0.75078  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[1800] - loss: 0.84149  lr: 0.00030  acc: 72.66% 93/128
Epoch[15] Batch[1900] - loss: 0.84531  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[2000] - loss: 1.07201  lr: 0.00030  acc: 65.62% 84/128
Epoch[15] Batch[2100] - loss: 0.68994  lr: 0.00030  acc: 79.69% 102/128
Epoch[15] Batch[2200] - loss: 0.61085  lr: 0.00030  acc: 81.25% 104/128
Epoch[15] Batch[2300] - loss: 0.84466  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[2400] - loss: 0.89657  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[2500] - loss: 0.93107  lr: 0.00030  acc: 67.19% 86/128
Epoch[15] Batch[2600] - loss: 0.73126  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[2700] - loss: 0.50143  lr: 0.00030  acc: 82.03% 105/128
Epoch[15] Batch[2800] - loss: 0.94762  lr: 0.00030  acc: 67.97% 87/128
Epoch[15] Batch[2900] - loss: 0.90799  lr: 0.00030  acc: 67.97% 87/128
Epoch[15] Batch[3000] - loss: 0.90370  lr: 0.00030  acc: 69.53% 89/128
Epoch[15] Batch[3100] - loss: 0.75478  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[3200] - loss: 0.92393  lr: 0.00030  acc: 67.97% 87/128
Epoch[15] Batch[3300] - loss: 0.94256  lr: 0.00030  acc: 66.41% 85/128
Epoch[15] Batch[3400] - loss: 0.81234  lr: 0.00030  acc: 78.12% 100/128
Epoch[15] Batch[3500] - loss: 0.61891  lr: 0.00030  acc: 81.25% 104/128
Epoch[15] Batch[3600] - loss: 0.74071  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[3700] - loss: 0.77173  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[3800] - loss: 0.97436  lr: 0.00030  acc: 68.75% 88/128
Epoch[15] Batch[3900] - loss: 0.87464  lr: 0.00030  acc: 68.75% 88/128
Epoch[15] Batch[4000] - loss: 0.87225  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[4100] - loss: 0.74723  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[4200] - loss: 0.67193  lr: 0.00030  acc: 80.47% 103/128
Epoch[15] Batch[4300] - loss: 0.78529  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[4400] - loss: 0.82455  lr: 0.00030  acc: 69.53% 89/128
Epoch[15] Batch[4500] - loss: 0.83746  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[4600] - loss: 0.76202  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[4700] - loss: 0.82459  lr: 0.00030  acc: 69.53% 89/128
Epoch[15] Batch[4800] - loss: 0.99394  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[4900] - loss: 0.71465  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[5000] - loss: 0.67124  lr: 0.00030  acc: 79.69% 102/128
Epoch[15] Batch[5100] - loss: 0.92240  lr: 0.00030  acc: 72.66% 93/128
Epoch[15] Batch[5200] - loss: 0.66988  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[5300] - loss: 0.85492  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[5400] - loss: 0.63401  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[5500] - loss: 0.83995  lr: 0.00030  acc: 70.31% 90/128
Epoch[15] Batch[5600] - loss: 0.77530  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[5700] - loss: 0.77694  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[5800] - loss: 0.84737  lr: 0.00030  acc: 72.66% 93/128
Epoch[15] Batch[5900] - loss: 0.66852  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[6000] - loss: 0.69429  lr: 0.00030  acc: 75.78% 97/128
Epoch[15] Batch[6100] - loss: 0.83987  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[6200] - loss: 0.95514  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[6300] - loss: 0.90472  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[6400] - loss: 0.77162  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[6500] - loss: 1.00211  lr: 0.00030  acc: 68.75% 88/128
Epoch[15] Batch[6600] - loss: 0.80244  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[6700] - loss: 0.74007  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[6800] - loss: 0.92127  lr: 0.00030  acc: 70.31% 90/128
Epoch[15] Batch[6900] - loss: 1.02511  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[7000] - loss: 0.87481  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[7100] - loss: 0.70274  lr: 0.00030  acc: 79.69% 102/128
Epoch[15] Batch[7200] - loss: 0.72580  lr: 0.00030  acc: 80.47% 103/128
Epoch[15] Batch[7300] - loss: 0.71941  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[7400] - loss: 0.65359  lr: 0.00030  acc: 79.69% 102/128
Epoch[15] Batch[7500] - loss: 0.85833  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[7600] - loss: 0.65092  lr: 0.00030  acc: 79.69% 102/128
Epoch[15] Batch[7700] - loss: 0.85507  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[7800] - loss: 0.75570  lr: 0.00030  acc: 75.00% 96/128
Epoch[15] Batch[7900] - loss: 0.87230  lr: 0.00030  acc: 71.09% 91/128
Epoch[15] Batch[8000] - loss: 0.76237  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[8100] - loss: 0.59991  lr: 0.00030  acc: 82.81% 106/128
Epoch[15] Batch[8200] - loss: 0.84598  lr: 0.00030  acc: 75.78% 97/128
Epoch[15] Batch[8300] - loss: 0.97597  lr: 0.00030  acc: 64.84% 83/128
Epoch[15] Batch[8400] - loss: 0.85254  lr: 0.00030  acc: 71.88% 92/128
Epoch[15] Batch[8500] - loss: 0.70679  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[8600] - loss: 0.80165  lr: 0.00030  acc: 81.25% 104/128
Epoch[15] Batch[8700] - loss: 0.92730  lr: 0.00030  acc: 68.75% 88/128
Epoch[15] Batch[8800] - loss: 0.78951  lr: 0.00030  acc: 75.78% 97/128
Epoch[15] Batch[8900] - loss: 0.92916  lr: 0.00030  acc: 67.97% 87/128
Epoch[15] Batch[9000] - loss: 0.78629  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[9100] - loss: 0.65255  lr: 0.00030  acc: 78.91% 101/128
Epoch[15] Batch[9200] - loss: 0.72517  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[9300] - loss: 0.75334  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[9400] - loss: 0.89632  lr: 0.00030  acc: 69.53% 89/128
Epoch[15] Batch[9500] - loss: 0.64825  lr: 0.00030  acc: 80.47% 103/128
Epoch[15] Batch[9600] - loss: 0.77298  lr: 0.00030  acc: 77.34% 99/128
Epoch[15] Batch[9700] - loss: 1.00975  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[9800] - loss: 0.80692  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[9900] - loss: 0.87219  lr: 0.00030  acc: 74.22% 95/128
Epoch[15] Batch[10000] - loss: 0.76316  lr: 0.00030  acc: 75.78% 97/128
Epoch[15] Batch[10100] - loss: 0.85402  lr: 0.00030  acc: 73.44% 94/128
Epoch[15] Batch[10200] - loss: 0.83352  lr: 0.00030  acc: 76.56% 98/128
Epoch[15] Batch[10300] - loss: 0.80292  lr: 0.00030  acc: 77.34% 99/128
=> saving checkpoint model to /hy-tmp/output/models_yahoo_answers/CharCNN_epoch_15.pth.tar

Evaluation - loss: 0.00681  lr: 0.00030  acc: 72.40 (50785/70144) error: 27.60
Label: [31m  0[0m	Prec: [32m 59.1[0m% (4296/7265)  Recall: [32m 60.6[0m% (4296/7094)  F-Score: [32m 59.8[0m%[0m
Label: [31m  1[0m	Prec: [32m 74.1[0m% (5233/7059)  Recall: [32m 74.3[0m% (5233/7042)  F-Score: [32m 74.2[0m%[0m
Label: [31m  2[0m	Prec: [32m 75.1[0m% (5848/7789)  Recall: [32m 82.4[0m% (5848/7095)  F-Score: [32m 78.6[0m%[0m
Label: [31m  3[0m	Prec: [32m 59.4[0m% (3393/5716)  Recall: [32m 49.3[0m% (3393/6888)  F-Score: [32m 53.8[0m%[0m
Label: [31m  4[0m	Prec: [32m 82.2[0m% (6267/7622)  Recall: [32m 88.3[0m% (6267/7095)  F-Score: [32m 85.2[0m%[0m
Label: [31m  5[0m	Prec: [32m 86.8[0m% (6251/7199)  Recall: [32m 88.8[0m% (6251/7042)  F-Score: [32m 87.8[0m%[0m
Label: [31m  6[0m	Prec: [32m 65.3[0m% (3294/5042)  Recall: [32m 47.3[0m% (3294/6969)  F-Score: [32m 54.8[0m%[0m
Label: [31m  7[0m	Prec: [32m 70.2[0m% (5164/7353)  Recall: [32m 73.8[0m% (5164/6993)  F-Score: [32m 72.0[0m%[0m
Label: [31m  8[0m	Prec: [32m 71.8[0m% (5476/7624)  Recall: [32m 79.2[0m% (5476/6913)  F-Score: [32m 75.3[0m%[0m
Label: [31m  9[0m	Prec: [32m 74.4[0m% (5563/7475)  Recall: [32m 79.3[0m% (5563/7013)  F-Score: [32m 76.8[0m%[0m




Epoch[16] Batch[100] - loss: 0.77774  lr: 0.00030  acc: 73.44% 94/128
Epoch[16] Batch[200] - loss: 0.76024  lr: 0.00030  acc: 76.56% 98/128
Epoch[16] Batch[300] - loss: 0.77827  lr: 0.00030  acc: 77.34% 99/128
Epoch[16] Batch[400] - loss: 0.56665  lr: 0.00030  acc: 82.03% 105/128
Epoch[16] Batch[500] - loss: 0.83568  lr: 0.00030  acc: 74.22% 95/128
Epoch[16] Batch[600] - loss: 0.68986  lr: 0.00030  acc: 79.69% 102/128
Epoch[16] Batch[700] - loss: 0.67457  lr: 0.00030  acc: 78.12% 100/128
Epoch[16] Batch[800] - loss: 0.67772  lr: 0.00030  acc: 78.91% 101/128
Epoch[16] Batch[900] - loss: 0.79581  lr: 0.00030  acc: 74.22% 95/128
Epoch[16] Batch[1000] - loss: 0.83469  lr: 0.00030  acc: 73.44% 94/128
Epoch[16] Batch[1100] - loss: 0.81068  lr: 0.00030  acc: 73.44% 94/128
Traceback (most recent call last):
  File "train.py", line 301, in <module>
    main()
  File "train.py", line 297, in main
    train(train_loader, dev_loader, model, args)
  File "train.py", line 136, in train
    optim.step()
  File "/usr/local/lib/python3.8/dist-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/usr/local/lib/python3.8/dist-packages/paddle/fluid/dygraph/base.py", line 276, in __impl__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/usr/local/lib/python3.8/dist-packages/paddle/fluid/wrapped_decorator.py", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/paddle/fluid/framework.py", line 227, in __impl__
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/adam.py", line 365, in step
    optimize_ops = self._apply_optimize(
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/optimizer.py", line 796, in _apply_optimize
    optimize_ops = self._create_optimization_pass(params_grads)
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/adamw.py", line 202, in _create_optimization_pass
    optimize_ops = super(
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/optimizer.py", line 623, in _create_optimization_pass
    self._append_optimize_op(target_block, param_and_grad)
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/adamw.py", line 198, in _append_optimize_op
    self._append_decoupled_weight_decay(block, param_and_grad)
  File "/usr/local/lib/python3.8/dist-packages/paddle/optimizer/adamw.py", line 173, in _append_decoupled_weight_decay
    with block.program._optimized_guard(
  File "/usr/local/lib/python3.8/dist-packages/decorator.py", line 231, in fun
    args, kw = fix(args, kw, sig)
  File "/usr/local/lib/python3.8/dist-packages/decorator.py", line 205, in fix
    return ba.args, ba.kwargs
  File "/usr/lib/python3.8/inspect.py", line 2668, in kwargs
    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
  File "/usr/local/lib/python3.8/dist-packages/paddle/fluid/multiprocess_utils.py", line 138, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid 8049) exited is killed by signal: Killed. (at /paddle/paddle/fluid/imperative/data_loader.cc:181)

